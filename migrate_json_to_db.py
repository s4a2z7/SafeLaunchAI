"""
SafeLaunch AI â€” JSON â†’ SQLite ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ database/*.json íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ìƒˆ ê´€ê³„í˜• DB(safelaunch.db)ë¡œ ì´ê´€í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python migrate_json_to_db.py              # ì‹¤í–‰ (dry-run ì•„ë‹˜)
    python migrate_json_to_db.py --dry-run    # ë¯¸ë¦¬ë³´ê¸°ë§Œ (DB ë³€ê²½ ì—†ìŒ)

ì´ê´€ ëŒ€ìƒ:
    - database/laws.json        â†’ laws + document_chunks í…Œì´ë¸”
    - database/precedents.json  â†’ precedents + document_chunks í…Œì´ë¸”
    - database/store_policies.json â†’ store_policies + document_chunks í…Œì´ë¸”

â€» ê¸°ì¡´ JSON íŒŒì¼ì€ ì‚­ì œí•˜ì§€ ì•Šìœ¼ë©° VectorStore(TF-IDF)ì™€ ë³‘í–‰ ìš´ì˜ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

import json
import os
import sys
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ íŒ¨ìŠ¤ì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(__file__))

from core.database import DatabaseManager, _new_id, _now
from core.store_policy_data import APPLE_POLICIES, GOOGLE_POLICIES

DATABASE_DIR = os.path.join(os.path.dirname(__file__), "database")


def load_json(filename: str) -> dict:
    """JSON íŒŒì¼ ë¡œë“œ"""
    path = os.path.join(DATABASE_DIR, filename)
    if not os.path.exists(path):
        print(f"  [SKIP] {filename} â€” íŒŒì¼ ì—†ìŒ")
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def migrate_laws(db: DatabaseManager, dry_run: bool = False) -> dict:
    """laws.json â†’ laws + document_chunks"""
    print("\n" + "=" * 50)
    print("ğŸ“š ë²•ë ¹ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("=" * 50)

    data = load_json("laws.json")
    if not data:
        return {"docs": 0, "chunks": 0, "skipped": 0}

    # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  ë²•ë ¹ ì¶”ì¶œ
    law_groups: dict[str, list] = {}  # law_id â†’ [chunks]
    for chunk_hash, chunk_data in data.items():
        meta = chunk_data.get("metadata", {})
        law_id = meta.get("law_id", "")
        if not law_id:
            continue
        if law_id not in law_groups:
            law_groups[law_id] = []
        law_groups[law_id].append({
            "hash": chunk_hash,
            "text": chunk_data.get("text", ""),
            "metadata": meta,
        })

    print(f"  ë°œê²¬: {len(law_groups)}ê°œ ë²•ë ¹, {len(data)}ê°œ ì²­í¬")

    if dry_run:
        for law_id, chunks in list(law_groups.items())[:3]:
            meta = chunks[0]["metadata"]
            print(f"    - {meta.get('law_name', '?')} (ID: {law_id}, {len(chunks)}ì²­í¬)")
        return {"docs": len(law_groups), "chunks": len(data), "skipped": 0}

    # ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜
    docs_count = 0
    chunks_count = 0

    for law_id, chunks in law_groups.items():
        meta = chunks[0]["metadata"]

        # ë²•ë ¹ ë ˆì½”ë“œ ìƒì„±
        db_law_id = db.upsert_law(
            law_id=law_id,
            law_name=meta.get("law_name", ""),
            proclamation_date=meta.get("proclamation_date"),
            enforcement_date=meta.get("enforcement_date"),
            source_url=meta.get("source_url"),
        )
        docs_count += 1

        # ì²­í¬ ìƒì„±
        for chunk in chunks:
            db.upsert_chunk(
                chunk_hash=chunk["hash"],
                source_type="law",
                source_id=db_law_id,
                chunk_index=chunk["metadata"].get("chunk_index", 0),
                content=chunk["text"],
                metadata=chunk["metadata"],
            )
            chunks_count += 1

    print(f"  ì™„ë£Œ: {docs_count}ê°œ ë²•ë ¹, {chunks_count}ê°œ ì²­í¬ ì´ê´€")
    return {"docs": docs_count, "chunks": chunks_count, "skipped": 0}


def migrate_precedents(db: DatabaseManager, dry_run: bool = False) -> dict:
    """precedents.json â†’ precedents + document_chunks"""
    print("\n" + "=" * 50)
    print("âš–ï¸  íŒë¡€ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("=" * 50)

    data = load_json("precedents.json")
    if not data:
        return {"docs": 0, "chunks": 0, "skipped": 0}

    # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  íŒë¡€ ì¶”ì¶œ
    prec_groups: dict[str, list] = {}
    for chunk_hash, chunk_data in data.items():
        meta = chunk_data.get("metadata", {})
        prec_seq = meta.get("precedent_seq", "")
        if not prec_seq:
            continue
        if prec_seq not in prec_groups:
            prec_groups[prec_seq] = []
        prec_groups[prec_seq].append({
            "hash": chunk_hash,
            "text": chunk_data.get("text", ""),
            "metadata": meta,
        })

    print(f"  ë°œê²¬: {len(prec_groups)}ê°œ íŒë¡€, {len(data)}ê°œ ì²­í¬")

    if dry_run:
        for prec_seq, chunks in list(prec_groups.items())[:3]:
            meta = chunks[0]["metadata"]
            print(f"    - {meta.get('case_name', '?')} ({meta.get('court_name', '?')}, {len(chunks)}ì²­í¬)")
        return {"docs": len(prec_groups), "chunks": len(data), "skipped": 0}

    docs_count = 0
    chunks_count = 0

    for prec_seq, chunks in prec_groups.items():
        meta = chunks[0]["metadata"]

        db_prec_id = db.upsert_precedent(
            precedent_seq=prec_seq,
            case_name=meta.get("case_name", ""),
            court_name=meta.get("court_name"),
            judgment_date=meta.get("judgment_date"),
            case_number=meta.get("case_number"),
            case_type=meta.get("case_type"),
            source_url=meta.get("source_url"),
        )
        docs_count += 1

        for chunk in chunks:
            db.upsert_chunk(
                chunk_hash=chunk["hash"],
                source_type="precedent",
                source_id=db_prec_id,
                chunk_index=chunk["metadata"].get("chunk_index", 0),
                content=chunk["text"],
                metadata=chunk["metadata"],
            )
            chunks_count += 1

    print(f"  ì™„ë£Œ: {docs_count}ê°œ íŒë¡€, {chunks_count}ê°œ ì²­í¬ ì´ê´€")
    return {"docs": docs_count, "chunks": chunks_count, "skipped": 0}


def migrate_store_policies(db: DatabaseManager, dry_run: bool = False) -> dict:
    """store_policies.json + í•˜ë“œì½”ë”© ì •ì±… â†’ store_policies + document_chunks"""
    print("\n" + "=" * 50)
    print("ğŸª ìŠ¤í† ì–´ ì •ì±… ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("=" * 50)

    # 1. í•˜ë“œì½”ë”©ëœ ì •ì±… ë°ì´í„° (store_policy_data.py)
    all_policies = APPLE_POLICIES + GOOGLE_POLICIES
    print(f"  í•˜ë“œì½”ë”© ì •ì±…: Apple {len(APPLE_POLICIES)}ê°œ + Google {len(GOOGLE_POLICIES)}ê°œ = {len(all_policies)}ê°œ")

    if dry_run:
        for p in all_policies[:3]:
            meta = p["metadata"]
            print(f"    - [{meta['store']}] {meta['section']} > {meta.get('subsection', '')}")
        return {"docs": len(all_policies), "chunks": 0, "skipped": 0}

    docs_count = 0
    chunks_count = 0

    for policy in all_policies:
        meta = policy["metadata"]
        db_policy_id = db.upsert_store_policy(
            store=meta["store"],
            section=meta["section"],
            subsection=meta.get("subsection", ""),
            content=policy["text"],
            policy_name=meta.get("policy_name", ""),
            effective_date=meta.get("effective_date"),
        )
        docs_count += 1

    # 2. JSON ì²­í¬ ë°ì´í„°
    data = load_json("store_policies.json")
    if data:
        for chunk_hash, chunk_data in data.items():
            meta = chunk_data.get("metadata", {})

            # ë§¤ì¹­ë˜ëŠ” ì •ì±…ì˜ DB IDë¥¼ ì°¾ê¸° ìœ„í•´ store+sectionìœ¼ë¡œ ê²€ìƒ‰
            with db.connection() as conn:
                row = conn.execute(
                    "SELECT id FROM store_policies WHERE store=? AND section=? LIMIT 1",
                    (meta.get("store", ""), meta.get("section", "")),
                ).fetchone()
                source_id = row["id"] if row else "unknown"

            db.upsert_chunk(
                chunk_hash=chunk_hash,
                source_type="store_policy",
                source_id=source_id,
                chunk_index=meta.get("chunk_index", 0),
                content=chunk_data.get("text", ""),
                metadata=meta,
            )
            chunks_count += 1

    print(f"  ì™„ë£Œ: {docs_count}ê°œ ì •ì±…, {chunks_count}ê°œ ì²­í¬ ì´ê´€")
    return {"docs": docs_count, "chunks": chunks_count, "skipped": 0}


def main():
    parser = argparse.ArgumentParser(description="JSON â†’ SQLite ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--dry-run", action="store_true", help="ë¯¸ë¦¬ë³´ê¸°ë§Œ (DB ë³€ê²½ ì—†ìŒ)")
    args = parser.parse_args()

    print("=" * 60)
    print("SafeLaunch AI â€” JSON â†’ SQLite ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("=" * 60)

    if args.dry_run:
        print("\nâš ï¸  DRY-RUN ëª¨ë“œ: DB ë³€ê²½ ì—†ì´ ë¯¸ë¦¬ë³´ê¸°ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n")
        db = None
    else:
        db = DatabaseManager()
        print(f"\në°ì´í„°ë² ì´ìŠ¤: {db.db_path}\n")

        # ë™ê¸°í™” ë¡œê·¸ ê¸°ë¡
        sync_id = db.start_sync(
            sync_type="full",
            queries=["json_migration"],
            triggered_by="migration_script",
        )

    # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    results = {}

    if args.dry_run:
        # dry-runì€ DB ì—†ì´ íŒŒì¼ë§Œ ë¶„ì„
        dummy_db = type("DummyDB", (), {})()
        results["laws"] = migrate_laws(dummy_db, dry_run=True)
        results["precedents"] = migrate_precedents(dummy_db, dry_run=True)
        results["store_policies"] = migrate_store_policies(dummy_db, dry_run=True)
    else:
        results["laws"] = migrate_laws(db)
        results["precedents"] = migrate_precedents(db)
        results["store_policies"] = migrate_store_policies(db)

        # ë™ê¸°í™” ì™„ë£Œ ê¸°ë¡
        total_docs = sum(r["docs"] for r in results.values())
        total_chunks = sum(r["chunks"] for r in results.values())
        db.complete_sync(
            sync_id=sync_id,
            items_added=total_docs,
            chunks_created=total_chunks,
        )

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"{'ì»¬ë ‰ì…˜':<20} {'ë¬¸ì„œ':>8} {'ì²­í¬':>8} {'ìŠ¤í‚µ':>8}")
    print("-" * 50)
    for name, r in results.items():
        print(f"{name:<20} {r['docs']:>8} {r['chunks']:>8} {r['skipped']:>8}")
    print("-" * 50)
    total_docs = sum(r["docs"] for r in results.values())
    total_chunks = sum(r["chunks"] for r in results.values())
    print(f"{'í•©ê³„':<20} {total_docs:>8} {total_chunks:>8}")

    if not args.dry_run:
        print(f"\në°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
        stats = db.get_stats()
        for key, val in stats.items():
            print(f"  {key}: {val}")

    print("\n" + "=" * 60)
    print("ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
